{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bite size #1 \n",
    "\n",
    "- Difference between binary cross-entropy and cross-entropy\n",
    "- how do they behave when they have 2 classes and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula of binary cross entropy loss\n",
    "\n",
    "$\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p(y_i)) + (1 - y_i) \\log(1 - p(y_i)) \\right]$\n",
    "\n",
    "Formula for multi-category cross entropy\n",
    "\n",
    "$\\text{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(p(y_{ik}))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0261140715602373, 1.0261140715602373, 1.3430935021291543)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.eye() --> \"eye\" is a play on words: it sounds like \"I\", which is commonly used to denote the identity matrix in linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# Generate random predictions and actual labels for N instances\n",
    "N = 1000\n",
    "y_pred = np.random.rand(N)  # Predicted probabilities for class 1\n",
    "y_actual = np.random.randint(0, 2, N)  # Actual labels (0 or 1)\n",
    "\n",
    "# Calculate Binary Cross-Entropy (BCE)\n",
    "bce_loss = -np.mean(y_actual * np.log(y_pred) + (1 - y_actual) * np.log(1 - y_pred))\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 2 classes\n",
    "y_pred_0 = 1 - y_pred  # Predicted probabilities for class 0\n",
    "y_actual_0 = 1 - y_actual  # Actual labels for class 0\n",
    "ce_loss_2class = -np.mean(y_actual * np.log(y_pred) + y_actual_0 * np.log(y_pred_0))\n",
    "\n",
    "# Generate predictions and actual labels for 3-class problem\n",
    "y_pred_3class = np.random.rand(N, 3)  # Predicted probabilities for 3 classes\n",
    "y_pred_3class = y_pred_3class / np.sum(y_pred_3class, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "y_actual_3class = np.random.randint(0, 3, N)  # Actual labels (0, 1, or 2)\n",
    "y_actual_3class_onehot = np.eye(3)[y_actual_3class]  # One-hot encode actual labels\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 3 classes\n",
    "ce_loss_3class = -np.mean(np.sum(y_actual_3class_onehot * np.log(y_pred_3class), axis=1))\n",
    "\n",
    "bce_loss, ce_loss_2class, ce_loss_3class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0261140715602373, 1.0261140715602373, 1.3430935021291541)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import xlogy\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculate Binary Cross-Entropy (BCE) using Scipy\n",
    "bce_loss_scipy = -np.mean(xlogy(y_actual, y_pred) + xlogy(1 - y_actual, 1 - y_pred))\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 2 classes using Scikit-learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "ce_loss_2class_sklearn = log_loss(y_actual, np.column_stack([y_pred_0, y_pred]))\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 3 classes using Scikit-learn\n",
    "ce_loss_3class_sklearn = log_loss(y_actual_3class, y_pred_3class)\n",
    "\n",
    "bce_loss_scipy, ce_loss_2class_sklearn, ce_loss_3class_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are only two classes (let's say class 0 and class 1), we can represent $y_0 = 1 - y_1$ and $y_1 = y$. Also, due to the softmax operation, $p_0 = 1-p$. Now we can rewrite the multi-class formula in terms of these variables and it would be the same as the binary cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite size #2\n",
    "- difference between the input of F.binary_cross_entropy and F.cross_entropy\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 0.3566749691963196\n",
      "Cross Entropy Loss: 0.22009485960006714\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Binary classification\n",
    "# Input is probability after sigmoid\n",
    "# input_prob = torch.tensor([1.7, 0.3], requires_grad=True) --> will get error\n",
    "input_prob = torch.tensor([0.7, 0.3], requires_grad=True)\n",
    "target = torch.tensor([1., 0.])\n",
    "loss_binary = F.binary_cross_entropy(input_prob, target)\n",
    "print(f\"Binary Cross Entropy Loss: {loss_binary.item()}\")\n",
    "\n",
    "# Multi-class classification\n",
    "# Input is raw logits before softmax\n",
    "input_logits = torch.tensor([[1.5, -0.5], [0.5, 1.5]], requires_grad=True)\n",
    "target = torch.tensor([0, 1])\n",
    "loss_cross = F.cross_entropy(input_logits, target)\n",
    "print(f\"Cross Entropy Loss: {loss_cross.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite size #3\n",
    "- how to define a custom callbacks in pytorch lightning to store tochmetrics Accuracy values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a custom callback to log validation accuracy at the end of each validation epoch. We utilizes the torchmetrics library to compute the accuracy. Callbacks allow to execute some code at a certain point via hooks.\n",
    "\n",
    "For a list of callbacks:\n",
    "https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "import torchmetrics\n",
    "\n",
    "val_metric = []\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # compute() calculates the value and returns a tensor\n",
    "        # .item() to get value of tensor\n",
    "        accuracy = pl_module.val_acc.compute().item()\n",
    "        val_metric.append(accuracy)\n",
    "        print(f\"val_metric: {val_metric}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite Size #4\n",
    "- to get model parameters\n",
    "- to get learning rate change by epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to retrieve a model's parameters using model.parameters() and track learning rate adjustments through a scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09997532801828658,\n",
       " 0.09990133642141358,\n",
       " 0.099778098230154,\n",
       " 0.0996057350657239,\n",
       " 0.09938441702975688]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# Print the size of each parameter in the model\n",
    "for param in model.parameters():\n",
    "    print(param.size())\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Create a list to store the learning rates obtained at each manual step\n",
    "manual_lrs = []\n",
    "\n",
    "# Record learning rates over 5 steps\n",
    "for _ in range(5):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    manual_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "# Output the learning rates recorded at each manual step\n",
    "manual_lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite Size #5\n",
    "- how to log metrics and parameters locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # to log locally via CSVLogger using Trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10, accelerator=\"cpu\", devices=1, deterministic=True,\n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    callbacks=[CustomCallback()]\n",
    ")\n",
    "\n",
    "# to define metrics to save with self.log\n",
    "class LightningModel(L.LightningModule):\n",
    "    ...\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.log(\"custom_loss\", loss)\n",
    "\n",
    "# to get directory of logs\n",
    "trainer.logger.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite Size #6\n",
    "- to to log callbacks metrics with CSVLogger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-competitions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
