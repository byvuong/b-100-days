{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bite size #1 \n",
    "\n",
    "- Difference between binary cross-entropy and cross-entropy\n",
    "- how do they behave when they have 2 classes and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula of binary cross entropy loss\n",
    "\n",
    "$\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p(y_i)) + (1 - y_i) \\log(1 - p(y_i)) \\right]$\n",
    "\n",
    "Formula for multi-category cross entropy\n",
    "\n",
    "$\\text{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(p(y_{ik}))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0261140715602373, 1.0261140715602373, 1.3430935021291543)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.eye() --> \"eye\" is a play on words: it sounds like \"I\", which is commonly used to denote the identity matrix in linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# Generate random predictions and actual labels for N instances\n",
    "N = 1000\n",
    "y_pred = np.random.rand(N)  # Predicted probabilities for class 1\n",
    "y_actual = np.random.randint(0, 2, N)  # Actual labels (0 or 1)\n",
    "\n",
    "# Calculate Binary Cross-Entropy (BCE)\n",
    "bce_loss = -np.mean(y_actual * np.log(y_pred) + (1 - y_actual) * np.log(1 - y_pred))\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 2 classes\n",
    "y_pred_0 = 1 - y_pred  # Predicted probabilities for class 0\n",
    "y_actual_0 = 1 - y_actual  # Actual labels for class 0\n",
    "ce_loss_2class = -np.mean(y_actual * np.log(y_pred) + y_actual_0 * np.log(y_pred_0))\n",
    "\n",
    "# Generate predictions and actual labels for 3-class problem\n",
    "y_pred_3class = np.random.rand(N, 3)  # Predicted probabilities for 3 classes\n",
    "y_pred_3class = y_pred_3class / np.sum(y_pred_3class, axis=1, keepdims=True)  # Normalize to sum to 1\n",
    "y_actual_3class = np.random.randint(0, 3, N)  # Actual labels (0, 1, or 2)\n",
    "y_actual_3class_onehot = np.eye(3)[y_actual_3class]  # One-hot encode actual labels\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 3 classes\n",
    "ce_loss_3class = -np.mean(np.sum(y_actual_3class_onehot * np.log(y_pred_3class), axis=1))\n",
    "\n",
    "bce_loss, ce_loss_2class, ce_loss_3class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0261140715602373, 1.0261140715602373, 1.3430935021291541)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import xlogy\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Calculate Binary Cross-Entropy (BCE) using Scipy\n",
    "bce_loss_scipy = -np.mean(xlogy(y_actual, y_pred) + xlogy(1 - y_actual, 1 - y_pred))\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 2 classes using Scikit-learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "ce_loss_2class_sklearn = log_loss(y_actual, np.column_stack([y_pred_0, y_pred]))\n",
    "\n",
    "# Calculate Cross-Entropy (CE) for 3 classes using Scikit-learn\n",
    "ce_loss_3class_sklearn = log_loss(y_actual_3class, y_pred_3class)\n",
    "\n",
    "bce_loss_scipy, ce_loss_2class_sklearn, ce_loss_3class_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are only two classes (let's say class 0 and class 1), we can represent $y_0 = 1 - y_1$ and $y_1 = y$. Also, due to the softmax operation, $p_0 = 1-p$. Now we can rewrite the multi-class formula in terms of these variables and it would be the same as the binary cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bite size #2\n",
    "- difference between the input of F.binary_cross_entropy and F.cross_entropy\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy Loss: 0.3566749691963196\n",
      "Cross Entropy Loss: 0.22009485960006714\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Binary classification\n",
    "# Input is probability after sigmoid\n",
    "# input_prob = torch.tensor([1.7, 0.3], requires_grad=True) --> will get error\n",
    "input_prob = torch.tensor([0.7, 0.3], requires_grad=True)\n",
    "target = torch.tensor([1., 0.])\n",
    "loss_binary = F.binary_cross_entropy(input_prob, target)\n",
    "print(f\"Binary Cross Entropy Loss: {loss_binary.item()}\")\n",
    "\n",
    "# Multi-class classification\n",
    "# Input is raw logits before softmax\n",
    "input_logits = torch.tensor([[1.5, -0.5], [0.5, 1.5]], requires_grad=True)\n",
    "target = torch.tensor([0, 1])\n",
    "loss_cross = F.cross_entropy(input_logits, target)\n",
    "print(f\"Cross Entropy Loss: {loss_cross.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-competitions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
